{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and filter the data by day\n",
    "For each month of data, load the data for each day and filter it first with a box that includes a region around Bogota and then with a shapefile with regions relevant to the study area that we care about (e.g. neighbhorhoods within Bogota). Write out that data in an efficient format (parquet) for calculation of user stats and filtering by user stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from plotting import * \n",
    "from preprocess import *\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "\n",
    "# Access environment variables and define other necessary variables\n",
    "data_dir = os.getenv('WORKING_DIR')\n",
    "meta_dir = f'{data_dir}metadata/'\n",
    "\n",
    "in_dir = f'{data_dir}data/'\n",
    "year = 'year=2019'\n",
    "data_year = f'{in_dir}{year}/'\n",
    "data_folders = glob.glob((data_year + '*/'))\n",
    "out_dir = f'{data_dir}data/parquet/in_study_area/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the shapefile and plot it to see if it is what we might expect for the study areas we want to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_name = 'union_utam_localidad_study_area'\n",
    "shapefile = f'{meta_dir}{shp_name}.shp'\n",
    "gdf_regions = gpd.read_file(shapefile)\n",
    "gdf_regions.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and sanity checks: \n",
    "For one day, filter the points by region and check the plots to ensure that the filtering is working as expected before applying the function to the whole dataset. \n",
    "- Time to run: ~ 2 minutes, including plotting functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and plot a fraction of all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one day for testing \n",
    "i = 0\n",
    "data_folder = data_folders[i]\n",
    "month = data_folder.split(data_year)[1].split('/')[0]\n",
    "day_dirs = get_days(data_folder)\n",
    "day_dir = day_dirs[i]\n",
    "\n",
    "filepaths, day = get_files(data_folder, day_dir)\n",
    "day_name = day.split('/')[0]\n",
    "ddf = load_data(filepaths, initial_cols, sel_cols, final_cols)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_obj, user_data = plot_frac_data_on_map(shapefile_path=shapefile, ddf=ddf, frac=0.0001)\n",
    "map_obj.save(f\"{data_dir}figures/{year}_{month}_{day_name}_user_pings_raw_f0001.html\")\n",
    "map_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and plot a fraction of the data limited to a box around the area of Bogota\n",
    "This reduces the data size quite substantially and quickly (by a factor of ~3 or so depending on how much data is outside the roughly defined region around Bogota)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_box_ddf = find_within_box(ddf, minlon, maxlon, minlat, maxlat)\n",
    "map_obj, user_data = plot_frac_data_on_map(shapefile_path=shapefile, ddf=within_box_ddf, frac=0.0001)\n",
    "map_obj.save(f\"{data_dir}figures/{year}_{month}_{day_name}_user_pings_bogbox_f0001.html\")\n",
    "map_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and plot a fraction of the data limited to the regions within the area of Bogota that are designated by our shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_in_regions = find_within_regions(ddf, gdf=gdf_regions)\n",
    "map_obj, user_data = plot_frac_data_on_map(shapefile_path=shapefile, ddf=ddf_in_regions, frac=0.01)\n",
    "map_obj.save(f\"{data_dir}figures/{year}_{month}_{day_name}_user_pings_in_studyareashp_f01.html\")\n",
    "map_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well. Let's make a function to more easily apply this filtering to our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function testing for the one day: \n",
    "ddf_in_regions = filter_data_for_day(filepaths, gdf=gdf_regions)\n",
    "ddf_in_regions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter all the days in the dataset for all months\n",
    "Because the previous functions appeared to filter the data in the desired way, we will now apply that filtering to the whole dataset\n",
    "- Time to run ~83 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data_folders)):\n",
    "    data_folder = data_folders[i]\n",
    "    from_month_write_filter_days_to_pq(data_folder, gdf=gdf_regions, out_dir=out_dir, data_year=data_year, year=year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the user stats based on the pings from the study area regions\n",
    "May take quite some time. \n",
    "- Time to run ~115 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mobilkit.stats\n",
    "\n",
    "def compute_user_stats_from_pq(pq_dir):\n",
    "    table_dd = dd.read_parquet(pq_dir, columns=['uid', 'datetime'])\n",
    "    user_stats = mobilkit.stats.userStats(table_dd).compute()\n",
    "    return user_stats\n",
    "\n",
    "user_stats = compute_user_stats_from_pq(out_dir)\n",
    "user_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write out user stats for the whole dataset for filtering\n",
    "Based on 60 ping and 10 day mininum cutoffs, kept 701961 of a total of 3557494 users for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(user_stats))\n",
    "output_filepath = f'{data_dir}/data/user_stats/user_stats_2019_allpings_months1-8s_shp_filtered.csv'\n",
    "user_stats.to_csv(output_filepath, index=False)\n",
    "#data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pings, min_days = 60, 10 \n",
    "output_filepath = f'{data_dir}/data/user_stats/user_stats_2019_months1-8_60min_pings_10min_days_shp_filtered.csv'\n",
    "\n",
    "# ran this once and will comment out for now to not overwrite the file: \n",
    "user_stats_filtered = user_stats[(user_stats['pings'] >= min_pings) & (user_stats['daysActive'] >= min_days)]\n",
    "print(f\"Based on {min_pings} ping and {min_days} day mininum cutoffs, kept {len(user_stats_filtered)} of a total of {len(user_stats)} users for this dataset.\")\n",
    "user_stats_filtered.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dataset by users that pass quality control \n",
    "Write out to parquet file for downstream analysis. (Reorganized the files into folders with 1 month of data each, except the first three months that are smaller in size.) \n",
    "- Time to run: ~14 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filepath = f'{data_dir}/data/user_stats/user_stats_2019_months1-8_60min_pings_10min_days_shp_filtered.csv'\n",
    "user_stats_filtered = pd.read_csv(output_filepath)\n",
    "user_stats_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq_dirs_months = glob.glob(out_dir + '*')\n",
    "pq_dirs_months_names = [i.split(f'{out_dir}')[1] for i in pq_dirs_months]\n",
    "uids_pass_qc= list(user_stats_filtered['uid'])\n",
    "data_for_qcd_users = f'{out_dir}bogota_study_area_year=2019_{pq_dirs_months_names[0]}_pass_qc.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "for i in tqdm(range(0,len(pq_dirs_months)), desc=f'Writing data for users that pass qc'):\n",
    "    print(f'Filtering data for {pq_dirs_months_names[i]}...')\n",
    "    dataset = ds.dataset(pq_dirs_months[i], format=\"parquet\")\n",
    "    table = dataset.to_table(filter=ds.field('uid').isin(uids_pass_qc))\n",
    "    # this causes the kernel to crash when I ran it on all the data so I need to rewrite it to not load everything in memory all at once\n",
    "    data_for_qcd_users = f'{out_dir}bogota_study_area_year=2019_{pq_dirs_months_names[i]}_pass_qc.parquet'\n",
    "    pq.write_table(table, data_for_qcd_users)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_mobile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
