{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPS location data from mobile phones to study impacts of Bogota cable car opening\n",
    "\n",
    "### Some background: \n",
    "- Flat part of bogota and some hills in the south that are now connected to the city center\n",
    "- Providers (private company) that collect mobile phone data at a large scale (buy data for all of Colombia from and focus on region within the city)\n",
    "- Now have two months worth of data for testing (have data from month after the opening of the cable car 2019, also a month from 2022)\n",
    "- Goal: Do analysis to see if the month of data is good enough to be worth buying 2018 data (more expensive)\n",
    "- How to know if it is good enough (2 key factors): How many observations appear in Bogota and what is their granularity and usefulness for tracking individual movements (e.g. can we see locations/movements between different neighborhoods?)\n",
    "\n",
    "### Tasks: \n",
    "1) CALCULATE TOTAL NUMBER OF OBSERVATIONS IN BOGOTA\n",
    "    - each observation is a ping, timestamped, individual identifiers for each device\n",
    "2) REGULARITY OF OBSERVATIONS PER DEVICE OVER TIME\n",
    "    - How long do I observe this device over the month? How often do I see each person per day?\n",
    "\n",
    "\n",
    "### Notes from 2023-03-08: \n",
    "- Drop users seen for less than 5 days and with less than 60 pings. Also for the home determination, maybe change the \"bed time\" to 10pm to 530am or 6am (try 11pm-5am and 10pm-6am).\n",
    "- Want to be able to determine the home locations of the people. \n",
    "\n",
    "#### Next steps:\n",
    "- Plots of user loss as the cutoffs for pings and days are varied.\n",
    "- Number of pings during hours when home locations could be computed (per user). How many people? (see above for cutoffs)\n",
    "- Map of home locations, especially see the regions in the south if we have some users from there. Generate a heatmap. Want to see some granularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import geopandas as gpd\n",
    "import skmob\n",
    "import contextily as cx\n",
    "import mobilkit\n",
    "import pytz\n",
    "from matplotlib import cm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from skmob.measures.individual import home_location\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import statsmodels.formula.api as smf\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "#set to working directory in an .env file\n",
    "%cd \"/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Elena gave me these potential column identifiers for the data: `['device_id', 'id_type', 'latitude', 'longitude', 'horizontal_accuracy',\n",
    "       'timestamp', 'ip_address', 'device_os', 'os_version', 'user_agent',\n",
    "       'country', 'source_id', 'publisher_id', 'app_id', 'location_context]`\n",
    "Looking at the data it looks like the columns are actually:\n",
    "`['device_id', 'id_type', 'latitude', 'longitude', 'horizontal_accuracy',\n",
    "       'timestamp', '?', 'device_os', 'os_version', 'user_agent',\n",
    "       'country', 'source_id', 'publisher_id', 'app_id', 'location_context]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_folder = \"./metadata/\"\n",
    "bogota_geo = meta_folder + \"poligonos-localidades.shp\"\n",
    "\n",
    "\n",
    "# read in the shapefile with the geolocation data for Bogota\n",
    "zats=gpd.read_file(bogota_geo)\n",
    "\n",
    "\n",
    "# get the first boundary component as a Polygon object to map Bogota\n",
    "boundary_bgta = Polygon(list(zats.geometry.unary_union.boundary.geoms)[0].coords) \n",
    "x, y = boundary_bgta.exterior.xy\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, y, color='dodgerblue', linewidth=1)\n",
    "plt.fill(x, y, color='aliceblue')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and filter for a box encompassing the larger area of Bogota\n",
    "\n",
    "Want to do this as a less computationally intensive way to quickly select the data that we may need/that may be of interest before proceeding, to make the computations more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepaths, initial_cols, sel_cols, final_cols): \n",
    "    \"\"\"Load in the mobile data and specify the columns\"\"\"\n",
    "    ddf = dd.read_csv(filepaths, names=initial_cols)\n",
    "    ddf = ddf[sel_cols]\n",
    "    ddf.columns = final_cols\n",
    "    return ddf \n",
    "\n",
    "def preprocess_mobile(df): #needs work\n",
    "    \"\"\"Process timestamp to datetime for dataframe with a \"datatime\" column with timestamp values.\"\"\"\n",
    "    df[\"datetime\"] = dd.to_datetime(df[\"datetime\"], unit='ms', errors='coerce')\n",
    "    df[\"datetime\"] = df[\"datetime\"].dt.tz_localize('UTC').dt.tz_convert('America/Bogota')\n",
    "    df = df.compute() #convert dask to pandas df after preprocessing\n",
    "    return df\n",
    "\n",
    "def get_days(data_folder):\n",
    "    \"\"\"Assuming a directory organized as a month's worth of days with files in each directory like \"day=01\", etc \"\"\"\n",
    "    day_dirs = glob.glob((data_folder + \"*/\"))\n",
    "    return day_dirs\n",
    "\n",
    "def get_files(data_folder, day_dir):\n",
    "    \"\"\"Assuming a dir corresponding to and named for a day day_dir, (e.g. \"day=01\") within the data_folder with that day's mobile data files.\"\"\"\n",
    "    day = day_dir.split(data_folder)[1]\n",
    "    filepaths = glob.glob((day_dir + '*[!*.gz]')) # select all the non-zipped mobile data files\n",
    "    print(day, filepaths)\n",
    "    return filepaths, day\n",
    "\n",
    "def find_within(df, boundaries): \n",
    "    gdf = gpd.GeoDataFrame(df.index, geometry=gpd.points_from_xy(np.array(df[\"lng\"]),np.array(df[\"lat\"])))\n",
    "    gdf = gdf.set_crs(epsg=4326)\n",
    "    ind = np.where(gdf.geometry.within(boundary_bgta))[0]\n",
    "    within_bound_df = df.iloc[ind,:].reset_index()\n",
    "    return within_bound_df\n",
    "\n",
    "def find_within_box(ddf, minlon , maxlon, minlat, maxlat):\n",
    "    \"\"\"Quick way to filter out points not in a particular rectangular region.\"\"\"\n",
    "    box=[minlon,minlat,maxlon,maxlat]\n",
    "    filtered_ddf = mobilkit.loader.crop_spatial(ddf, box).reset_index()\n",
    "    return filtered_ddf\n",
    "\n",
    "def write_within_bounds(ddf, output_folder, day):\n",
    "    ddf.compute().to_csv((output_folder + '/' + day.split('/')[0] + '_within_bogota.csv'), sep=',')\n",
    "    \n",
    "data_folder = \"/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/year=2019/month=1/\"\n",
    "day_dirs = get_days(data_folder)\n",
    "\n",
    "initial_cols=['device_id', 'id_type', 'latitude', 'longitude', 'horizontal_accuracy', 'timestamp',  'ip_address', 'device_os', 'country', 'unknown_2', 'geohash']\n",
    "sel_cols = [\"device_id\",\"latitude\",\"longitude\",\"timestamp\",\"geohash\",\"horizontal_accuracy\"]\n",
    "final_cols = [\"user_id\",\"lat\",\"lng\",\"datetime\",\"geohash\",\"horizontal_accuracy\"]\n",
    "output_folder = \"/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/bogota_box_2019\"\n",
    "\n",
    "# boundary box that roughly captures the larger county of Bogota\n",
    "minlon = -74.453\n",
    "maxlon = -73.992\n",
    "minlat = 3.727\n",
    "maxlat = 4.835\n",
    "\n",
    "for i in range(0,len(day_dirs)): \n",
    "    print(i)\n",
    "    day_dir = day_dirs[i]\n",
    "    filepaths, day = get_files(data_folder, day_dir)\n",
    "    ddf = load_data(filepaths, initial_cols, sel_cols, final_cols)\n",
    "    ddf = find_within_box(ddf, minlon , maxlon, minlat, maxlat)\n",
    "    write_within_bounds(ddf, output_folder, day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data in the box around Bogota and further filter to find the pings that were a) detected near the stations of interest and b) within the neighborhoods of interest \n",
    "\n",
    "We want to restrict the users to the areas with pings near the stations as dinlineated by the shapefile of Bogota that Elena gave me and label the pings with the station/neighborhood information as well. We also want to find the users that may live near the stations to see if their travel patterns change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data from within 1km of the stations of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/bogota_1km_stations_2019/\"\n",
    "shapefile_path = meta_folder + \"Stations_Buffer_1000.shp\" #\"poligonos-localidades.shp\"\n",
    "\n",
    "def find_within(shapefile_path, df):\n",
    "\n",
    "    # read in the shapefile containing the regions\n",
    "    regions = gpd.read_file(shapefile_path)\n",
    "    \n",
    "    # create a geopandas GeoDataFrame from the Dash DataFrame\n",
    "    geometry = gpd.points_from_xy(df.lng, df.lat)\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "\n",
    "    # perform a spatial join to keep only the rows within the regions\n",
    "    filtered_gdf = gpd.sjoin(gdf, regions, op='within')\n",
    "\n",
    "    # convert back to a pandas DataFrame for use in Dash\n",
    "    filtered_df = filtered_gdf.drop('geometry', axis=1).reset_index(drop=True)\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/bogota_1km_stations_2019/\"\n",
    "\n",
    "filepaths = glob.glob((data_folder + \"*.csv\"))\n",
    "\n",
    "for i in range(1,len(filepaths)): #len(day_dirs)\n",
    "    filepath = filepaths[i]\n",
    "    day = filepath.split(data_folder)[1].split('_within_bogota.csv')[0]\n",
    "    ddf = dd.read_csv(filepath)[final_cols]\n",
    "    df = ddf.compute()\n",
    "    filtered_df = find_within(shapefile_path, df)\n",
    "    filtered_df.to_csv(f\"{output_folder}/{day}_regions_labelled.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data from within the neighborhoods from the file of Bogota that Elena shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/bogota_localidades_2019/\"\n",
    "shapefile_path = meta_folder + \"poligonos-localidades.shp\"\n",
    "\n",
    "filepaths = glob.glob((data_folder + \"*.csv\"))\n",
    "\n",
    "for i in range(0,len(filepaths)): #len(day_dirs)\n",
    "    filepath = filepaths[i]\n",
    "    day = filepath.split(data_folder)[1].split('_within_bogota.csv')[0]\n",
    "    ddf = dd.read_csv(filepath)[final_cols]\n",
    "    df = ddf.compute()\n",
    "    filtered_df = find_within(shapefile_path, df)\n",
    "    filtered_df.to_csv(f\"{output_folder}/{day}_regions_labelled.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data for the pings near the stations to detect users with home locations nearby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/bogota_1km_stations_2019/\"\n",
    "users_w_ping_near_station_filepath = '/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/jan2019_users_w_ping_bogota_1km_stations.csv'\n",
    "\n",
    "def get_users(data_folder, output_filepath, user_col=\"user_id\"):\n",
    "    filepaths = glob.glob((data_folder + \"*.csv\"))\n",
    "    ddf = dd.read_csv(filepaths)\n",
    "    print(f\"There are {len(ddf)} pings from {len(ddf[user_col].value_counts().index)} unique users from this dataset.\")\n",
    "    \n",
    "    # write out the users that have a ping near one of the stations and use this list to filter the data for all the pings within the Bogota region\n",
    "    users = pd.DataFrame({user_col: list(ddf[user_col].value_counts().index)})\n",
    "    users.to_csv(output_filepath, index=False)\n",
    "\n",
    "get_users(data_folder, users_w_ping_near_station_filepath)\n",
    "\n",
    "users_w_ping_near_station = pd.read_csv(users_w_ping_near_station_filepath)\n",
    "users_w_ping_near_station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from the bogota area and restrict it to those users with at least one ping in the area of one of the stations\n",
    "def get_user_list(user_file, user_col=\"user_id\"):\n",
    "    \"\"\"Takes in a file (user_file) with a single column of user ids and exports a list of those users\"\"\"\n",
    "    users = pd.read_csv(user_file)\n",
    "    return list(users[user_col])\n",
    "\n",
    "def write_filter_data_by_users(data_folder: str, users: list, output_filepath: str, user_col=\"user_id\"):\n",
    "    \"\"\"Takes a dataframe and filters it to only include data from the specified users.\"\"\"\n",
    "    filepaths = glob.glob((data_folder + \"*.csv\"))\n",
    "    ddf = dd.read_csv(filepaths)\n",
    "    ddf = ddf[cols_to_keep]\n",
    "    users_num_og = len(ddf[user_col].value_counts().index)\n",
    "    df_specified_users = ddf[ddf[user_col].isin(users)].compute().reset_index()\n",
    "    users_num_post = len(df_specified_users[user_col].value_counts().index)\n",
    "    df_specified_users.to_csv(output_filepath, index=False)\n",
    "    print(f\"Wrote data for the {users_num_post} of a total of {users_num_og} users for this dataset.\")\n",
    "    #return df_specified_users\n",
    "\n",
    "cols_to_keep = ['user_id', 'lat', 'lng', 'datetime', 'geohash', 'horizontal_accuracy', 'Nombre_de_l', 'Identificad']\n",
    "data_folder = \"/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/bogota_localidades_2019/\"\n",
    "user_filtered_data_filepath = '/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/jan2019_bogota_data_for_users_w_ping_1km_stations.csv'\n",
    "\n",
    "users = get_user_list(users_w_ping_near_station_filepath)\n",
    "write_filter_data_by_users(data_folder, users, user_filtered_data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data for the bogota users for whom we detected at least one ping near a station of interest for further filtering \n",
    "def calculate_filter_user_stats(data_filepath: str, cols: list, renamed_cols: list, output_filepath: str, min_pings: int, min_days: int): \n",
    "    ddf = dd.read_csv(data_filepath)\n",
    "    ddf = ddf[cols]\n",
    "    ddf.columns = renamed_cols\n",
    "    ddf['datetime']=dd.to_datetime(ddf['datetime'],unit='ms', errors='coerce')\n",
    "    ddf[\"datetime\"]=ddf[\"datetime\"].dt.tz_localize('UTC').dt.tz_convert('America/Bogota')\n",
    "    users_stats_df = mobilkit.stats.userStats(ddf).compute()\n",
    "    users_stats_df_filtered = users_stats_df[(users_stats_df['pings'] >= min_pings) & (users_stats_df['daysActive'] >= min_days)]\n",
    "    print(f\"Based on {min_pings} ping and {min_days} day mininum cutoffs, kept {len(users_stats_df_filtered)} of a total of {len(users_stats_df)} users for this dataset.\")\n",
    "    users_stats_df_filtered.to_csv(output_filepath, index=False)\n",
    "    mobilkit.stats.plotUsersHist(users_stats_df, min_pings=min_pings, min_days=min_days)\n",
    "\n",
    "output_folder = \"/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/\"\n",
    "min_pings, min_days = 60, 10\n",
    "renamed_cols = ['uid', 'lat', 'lng', 'datetime', 'geohash', 'horizontal_accuracy', 'Nombre_de_l', 'Identificad']\n",
    "user_dataquantity_filtered_filepath = f'{output_folder}jan2019_users_w_ping_bogota_1km_stations_{min_pings}minpings_{min_days}mindays_in_bogota_region.csv'\n",
    "calculate_filter_user_stats(user_filtered_data_filepath, cols_to_keep, renamed_cols, user_dataquantity_filtered_filepath, min_pings=min_pings, min_days=min_days) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data for this final set of users to compute which ones have home locations near the stations: \n",
    "users = get_user_list(user_dataquantity_filtered_filepath, user_col='uid')\n",
    "data_folder = \"/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/bogota_localidades_2019/\"\n",
    "user_dataquantity_filtered_data_filepath = f'/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/jan2019_bogota_data_for_users_w_ping_1km_stations_{min_pings}pings_{min_days}days.csv'\n",
    "write_filter_data_by_users(data_folder, users, user_dataquantity_filtered_data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_preprocess_dash(data_filepath, cols, renamed_cols):\n",
    "    ddf = dd.read_csv(user_dataquantity_filtered_data_filepath)\n",
    "    ddf = ddf[cols]\n",
    "    ddf.columns = renamed_cols\n",
    "    ddf['datetime']=dd.to_datetime(ddf['datetime'],unit='ms', errors='coerce')\n",
    "    ddf[\"datetime\"]=ddf[\"datetime\"].dt.tz_localize('UTC').dt.tz_convert('America/Bogota')\n",
    "    return ddf \n",
    "\n",
    "ddf = read_preprocess_dash(user_dataquantity_filtered_data_filepath, cols_to_keep, renamed_cols)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile = meta_folder + \"Stations_Buffer_1000.shp\"\n",
    "\n",
    "def find_home_work_locs(ddf, shapefile, home_hrs=(22.0, 6.0), work_hrs=(9.5,16.5)):\n",
    "    regions = gpd.read_file(shapefile)\n",
    "    ddf_w_zones, tessellation_gdf = mobilkit.spatial.tessellate(ddf,tesselation_shp=shapefile,filterAreas=True)\n",
    "    ddf_w_zones = mobilkit.stats.userHomeWork(ddf_w_zones,\n",
    "                                         homeHours=home_hrs,\n",
    "                                         workHours=work_hrs)\n",
    "    #these next two actually detect home locations and may take some time\n",
    "    ddf_w_zones_stat= mobilkit.stats.userHomeWorkLocation(ddf_w_zones)\n",
    "    df_hw_locs = ddf_w_zones_stat.compute()\n",
    "    return ddf_w_zones, tessellation_gdf, df_hw_locs\n",
    "\n",
    "ddf_w_zones, tessellation_gdf, df_hw_locs = find_home_work_locs(ddf, shapefile, home_hrs=(22.0, 6.0), work_hrs=(9.5,16.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessellation_gdf_w_home =pd.merge(tessellation_gdf,df_hw_locs.groupby(by=\"home_tile_ID\").count().reset_index()[[\"home_tile_ID\",\"home_pings\"]],left_on=\"tile_ID\",right_on=\"home_tile_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets count and visualize the number of users that live near the stations\n",
    "\n",
    "Make nicer plots later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The breakdown of users living by different stations that we can detect is as follows:\\n', tessellation_gdf_w_home[['Station', 'home_pings']])\n",
    "print(f\"There are {tessellation_gdf_w_home['home_pings'].sum()} users that live near the stations that pass the filtering conditions for min pings and days detected.\")\n",
    "tessellation_gdf_w_home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(10,10))\n",
    "ax.set_ylim([4.3,5])\n",
    "ax.set_ylim([4.5,4.6])\n",
    "ax.set_xlim([-74.2,-74.05])\n",
    "tessellation_gdf_w_home.plot(column=\"home_pings\",ax=ax,legend=True,vmax=600)\n",
    "plt.savefig('./station_zones_map_home_10pm_6am_jan2019_accurate.png', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_for_users_living_in_specified_regions(df, output_file_path_no_ext: str): \n",
    "    users_w_home_tile = df[df.home_tile_ID.notnull()]\n",
    "    users_w_home_tile['uid'] = users_w_home_tile.index\n",
    "    users_w_home_tile.to_csv(f\"{output_file_path_no_ext}_w_hometile.csv\",index=False)\n",
    "    users_w_home_ping = df[df.home_pings > 0]\n",
    "    users_w_home_ping['uid'] = users_w_home_ping.index\n",
    "    users_w_home_ping.to_csv(f\"{output_file_path_no_ext}_w_homeping.csv\",index=False)\n",
    "    print(f'Out of {len(df)} users, {len(users_w_home_tile)} have a home tile ID assigned, and {len(users_w_home_ping)} have a home ping in the regions specified.')\n",
    "    return\n",
    "\n",
    "output_file_path_no_ext = f'/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/jan2019_bogota_users_w_ping_1km_stations_{min_pings}pings_{min_days}days'\n",
    "filter_for_users_living_in_specified_regions(df_hw_locs, output_file_path_no_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the final data for these users to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/bogota_localidades_2019/\"\n",
    "output_file_path_no_ext = f'/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/jan2019_bogota_users_w_ping_1km_stations_{min_pings}pings_{min_days}days'\n",
    "min_pings, min_days = 60, 10\n",
    "\n",
    "# filter the data for this final set of users to compute which ones have home locations near the stations: \n",
    "final_user_hometile_filepath = output_file_path_no_ext + '_w_hometile.csv'\n",
    "users = get_user_list(final_user_hometile_filepath, user_col='uid')\n",
    "final_filtered_hometile_filepath = f'/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/jan2019_bogota_users_w_ping_1km_stations_{min_pings}pings_{min_days}days_w_hometile.csv'\n",
    "#write_filter_data_by_users(data_folder, users, final_filtered_hometile_filepath)\n",
    "\n",
    "\n",
    "#final_user_homeping_filepath = output_file_path_no_ext + '_w_homeping.csv'\n",
    "users = get_user_list(final_user_homeping_filepath, user_col='uid')\n",
    "final_filtered_homeping_filepath = f'/Users/emilyrobitschek/git/ETH/SPUR/mobile_data_colombia/data/jan2019_bogota_users_w_ping_1km_stations_{min_pings}pings_{min_days}days_w_homeping.csv'\n",
    "#write_filter_data_by_users(data_folder, users, final_filtered_homeping_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('geo_mobile')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "94266dc0016789288e396fdf5aae0a4b8003dfb3304fe52aa146b44e3e043f3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
