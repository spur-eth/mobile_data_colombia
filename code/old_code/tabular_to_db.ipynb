{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion of tabular data to other formats for querying \n",
    "The purpose of this notebook is to test (memory, time) efficient methods to work with big data that don't fit into memory for queries and to run downstream analyses. In this case the data are large tabular files of mobile data. \n",
    "The two methods I will try are: \n",
    "\n",
    "1) To create a database [with SQLite](https://www.sqlite.org/index.html) (in this notebook)\n",
    "2) To re-write the data to a parquet format using [Apache Arrow](https://arrow.apache.org/docs/python/parquet.html) (in the `tabular_to_parquet.ipynb` notebook)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reading and package imports shared across methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import glob\n",
    "#from tqdm import tqdm, notebook\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import mobilkit #.loader.crop_spatial as mkcrop_spatial\n",
    "\n",
    "# Access environment variables and define other necessary variables\n",
    "data_dir = os.getenv('WORKING_DIR')\n",
    "meta_dir = f'{data_dir}metadata/'\n",
    "\n",
    "data_2019 = f'{data_dir}data/year=2019/'\n",
    "data_folders = glob.glob((data_2019 + '*/'))\n",
    "print(data_folders)\n",
    "\n",
    "initial_cols=['device_id', 'id_type', 'latitude', 'longitude', 'horizontal_accuracy', 'timestamp',  'ip_address', 'device_os', 'country', 'unknown_2', 'geohash']\n",
    "sel_cols = [\"device_id\",\"latitude\",\"longitude\",\"timestamp\",\"geohash\",\"horizontal_accuracy\"]\n",
    "final_cols = [\"uid\",\"lat\",\"lng\",\"datetime\",\"geohash\",\"horizontal_accuracy\"]\n",
    "\n",
    "# boundary box that roughly captures the larger county of Bogota\n",
    "minlon = -74.453\n",
    "maxlon = -73.992\n",
    "minlat = 3.727\n",
    "maxlat = 4.835\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FUNCTIONS FOR DATA PROCESSING ####\n",
    "\n",
    "def get_days(data_folder):\n",
    "    \"\"\"Assuming a directory organized as a month's worth of days with files in each directory like \"day=01\", etc \"\"\"\n",
    "    day_dirs = glob.glob((data_folder + '*/'))\n",
    "    return day_dirs\n",
    "\n",
    "def get_files(data_folder, day_dir):\n",
    "    \"\"\"Assuming a dir corresponding to and named for a day day_dir, (e.g. \"day=01\") within the data_folder with that day's mobile data files.\"\"\"\n",
    "    day = day_dir.split(data_folder)[1]\n",
    "    filepaths = glob.glob((day_dir + '*[!*.gz]')) # select all the non-zipped mobile data files\n",
    "    return filepaths, day\n",
    "\n",
    "def load_data(filepaths, initial_cols, sel_cols, final_cols): \n",
    "    \"\"\"Load in the mobile data and specify the columns\"\"\"\n",
    "    ddf = dd.read_csv(filepaths, names=initial_cols)\n",
    "    ddf = ddf[sel_cols]\n",
    "    ddf.columns = final_cols\n",
    "    return ddf \n",
    "\n",
    "def convert_datetime(ddf: dd.DataFrame): #needs work\n",
    "    \"\"\"Process timestamp to datetime for dataframe with a \"datatime\" column with timestamp values. \"\"\"\n",
    "    ddf[\"datetime\"] = dd.to_datetime(ddf[\"datetime\"], unit='ms', errors='coerce')\n",
    "    ddf[\"datetime\"] = ddf[\"datetime\"].dt.tz_localize('UTC').dt.tz_convert('America/Bogota')\n",
    "    return ddf\n",
    "\n",
    "def preprocess_mobile(ddf: dd.DataFrame, final_cols: list, minlon , maxlon, minlat, maxlat): #needs work\n",
    "    \"\"\"Select only those points within an area of interest and process timestamp to datetime \n",
    "    for dataframe with a \"datatime\" column with timestamp values.\"\"\"\n",
    "    ddf = find_within_box(ddf, minlon, maxlon, minlat, maxlat)\n",
    "    ddf = convert_datetime(ddf)[final_cols]\n",
    "    df = ddf.compute()\n",
    "    return df\n",
    "\n",
    "def find_within_box(ddf, minlon, maxlon, minlat, maxlat):\n",
    "    \"\"\"Quick way to filter out points not in a particular rectangular region.\"\"\"\n",
    "    box=[minlon,minlat,maxlon,maxlat]\n",
    "    filtered_ddf = mobilkit.loader.crop_spatial(ddf, box).reset_index()\n",
    "    return filtered_ddf\n",
    "\n",
    "#### FUNCTIONS FOR DATABASE ####\n",
    "\n",
    "def link_database(dir: str, db_name: str):\n",
    "    db_path = f'{dir}{db_name}.db'\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    return db_path, conn\n",
    "\n",
    "def add_data_to_db(df, conn, table_name, if_exists='replace'): \n",
    "    \"\"\"Add data to database. The options for 'if_exists' include appending ('append') the data or replacing it ('replace')\"\"\"\n",
    "    df.to_sql(table_name, conn, if_exists=if_exists, index=False)\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# specify database location and name and create database\n",
    "input_dir = f'{data_dir}data/'\n",
    "db_name = 'bogota_mobile_raw'\n",
    "db_path, conn = link_database(dir=input_dir, db_name=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'in_Bogota_area'\n",
    "\n",
    "for i in range(0, len(data_folders)):\n",
    "    data_folder = data_folders[i]\n",
    "    day_dirs = glob.glob((data_folder + '*/'))\n",
    "    for j in trange(0,len(day_dirs)): \n",
    "        day_dir = day_dirs[j]\n",
    "        filepaths, day = get_files(data_folder, day_dir)\n",
    "        ddf = load_data(filepaths, initial_cols, sel_cols, final_cols)\n",
    "        df = preprocess_mobile(ddf, final_cols, minlon, maxlon, minlat, maxlat)\n",
    "        add_data_to_db(df, conn, table_name, if_exists='append')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a query and return the result as a DataFrame\n",
    "def query_database_ids(query):\n",
    "    return pd.read_sql_query(query, conn)\n",
    "\n",
    "# Example query: Retrieve all unique values in the 'device_id' field\n",
    "query = f\"SELECT DISTINCT uid FROM {table_name}\"\n",
    "result = query_database_ids(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values as a list\n",
    "unique_device_ids = result['uid'].tolist()\n",
    "unique_device_ids_test = unique_device_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a query on the database and retrieve the result as a DataFrame\n",
    "def query_database(query, conn):\n",
    "    return pd.read_sql_query(query, conn, parse_dates='datetime')\n",
    "\n",
    "\n",
    "# Example query: Retrieve all rows for a specific device_id\n",
    "example_id = '84c28cbf-19bd-4a46-b528-197516925af7'\n",
    "device_id = example_id\n",
    "query = f\"SELECT * FROM {table_name} WHERE uid = '{device_id}'\"\n",
    "result = query_database(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dd = dd.from_pandas(result, npartitions=2)\n",
    "users_stats_df = mobilkit.stats.userStats(result_dd).compute()\n",
    "users_stats_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = ['total_user_pings', 'daysActive']\n",
    "\n",
    "# Check if 'new_column' already exists in the table\n",
    "def check_columns_db(conn, table_name: str, col: str):\n",
    "    existing_columns = query_database(f\"PRAGMA table_info({table_name})\", conn)\n",
    "    if col not in existing_columns['name'].values:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"ALTER TABLE {table_name} ADD COLUMN {col} REAL\")\n",
    "        cursor.close()\n",
    "    return \n",
    "\n",
    "for i in trange(0, len(new_cols)): \n",
    "    check_columns_db(conn, table_name, col=new_cols[i])\n",
    "\n",
    "# Add the new columns back to the database\n",
    "result['total_user_pings'], result['daysActive'] = users_stats_df['pings'], users_stats_df['daysActive']\n",
    "result.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_stats(conn, table_name, uid, npartiions=2):\n",
    "    query = f\"SELECT * FROM {table_name} WHERE uid = '{uid}'\"\n",
    "    result = query_database(query, conn)\n",
    "    result_dd = dd.from_pandas(result, npartitions=npartiions)\n",
    "    users_stats_df = mobilkit.stats.userStats(result_dd).compute()\n",
    "    result['total_user_pings'], result['daysActive'] = users_stats_df['pings'], users_stats_df['daysActive']\n",
    "    result.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "    return result\n",
    "\n",
    "for i in trange(0, len(unique_device_ids_test)): \n",
    "    uid = unique_device_ids_test[i]\n",
    "    users_stats_to_add = get_user_stats(conn, table_name, uid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = unique_device_ids_test[5]\n",
    "test = get_user_stats(conn, table_name, uid)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a query and return the result as a DataFrame\n",
    "def query_database_ids(query):\n",
    "    return pd.read_sql_query(query, conn)\n",
    "\n",
    "# Example query: Retrieve all unique values in the 'device_id' field\n",
    "query = f\"SELECT DISTINCT total_user_pings FROM {table_name}\"\n",
    "test2 = query_database_ids(query)\n",
    "test2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note from July 2023: \n",
    "Overall, this accessing and writing to the database using this method doesn't seems as efficient as the parquet method (although maybe both could be combined). I'll focus on the parquet method for the time being (in the tabular_to_parquet.ipynb notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('geo_mobile')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94266dc0016789288e396fdf5aae0a4b8003dfb3304fe52aa146b44e3e043f3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
