{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion of tabular data to other formats for querying \n",
    "The purpose of this notebook is to test (memory, time) efficient methods to work with big data that don't fit into memory for queries and to run downstream analyses. In this case the data are large tabular files of mobile data. \n",
    "The two methods I will try are: \n",
    "\n",
    "1) To create a database [with SQLite](https://www.sqlite.org/index.html)\n",
    "2) To re-write the data to a parquet format using [Apache Arrow](https://arrow.apache.org/docs/python/parquet.html)\n",
    "\n",
    "(The second option honestly seems much faster and better than the first, at least so far in my hands.)\n",
    "\n",
    "other resources to look into: \n",
    "- [Apache Beam](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/sql_taxi.py)\n",
    "- [mobilekit POI mapping](https://mobilkit.readthedocs.io/en/latest/examples/M4R_03_POI_visit_analysis.html) and related [docs](https://mobilkit.readthedocs.io/en/latest/mobilkit.spatial.html)\n",
    "- [info](https://docs.dask.org/en/stable/generated/dask.dataframe.read_parquet.html) about reading parquet files with dask \n",
    "\n",
    "## Filter of data by qc parameters \n",
    "Will also calculate user stats and filter data for users that meet the minimum standard for downstream analysis. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data reading and package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "import glob\n",
    "#from tqdm import tqdm, notebook\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import mobilkit #.loader.crop_spatial as mkcrop_spatial\n",
    "\n",
    "# Access environment variables and define other necessary variables\n",
    "data_dir = os.getenv('WORKING_DIR')\n",
    "meta_dir = f'{data_dir}metadata/'\n",
    "\n",
    "data_2019 = f'{data_dir}data/year=2019/'\n",
    "data_folders = glob.glob((data_2019 + '*/'))\n",
    "\n",
    "initial_cols=['device_id', 'id_type', 'latitude', 'longitude', 'horizontal_accuracy', 'timestamp',  'ip_address', 'device_os', 'country', 'unknown_2', 'geohash']\n",
    "sel_cols = [\"device_id\",\"latitude\",\"longitude\",\"timestamp\",\"geohash\",\"horizontal_accuracy\"]\n",
    "final_cols = [\"uid\",\"lat\",\"lng\",\"datetime\",\"geohash\",\"horizontal_accuracy\"]\n",
    "\n",
    "# boundary box that roughly captures the larger county of Bogota\n",
    "minlon = -74.453\n",
    "maxlon = -73.992\n",
    "minlat = 3.727\n",
    "maxlat = 4.835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FUNCTIONS FOR DATA PROCESSING ####\n",
    "\n",
    "def get_days(data_folder):\n",
    "    \"\"\"Assuming a directory organized as a month's worth of days with files in each directory like \"day=01\", etc \"\"\"\n",
    "    day_dirs = glob.glob((data_folder + '*/'))\n",
    "    return day_dirs\n",
    "\n",
    "def get_files(data_folder, day_dir):\n",
    "    \"\"\"Assuming a dir corresponding to and named for a day day_dir, (e.g. \"day=01\") within the data_folder with that day's mobile data files.\"\"\"\n",
    "    day = day_dir.split(data_folder)[1]\n",
    "    filepaths = glob.glob((day_dir + '*[!*.gz]')) # select all the non-zipped mobile data files\n",
    "    return filepaths, day\n",
    "\n",
    "def load_data(filepaths, initial_cols, sel_cols, final_cols): \n",
    "    \"\"\"Load in the mobile data and specify the columns\"\"\"\n",
    "    ddf = dd.read_csv(filepaths, names=initial_cols)\n",
    "    ddf = ddf[sel_cols]\n",
    "    ddf.columns = final_cols\n",
    "    return ddf \n",
    "\n",
    "def convert_datetime(ddf: dd.DataFrame): #needs work\n",
    "    \"\"\"Process timestamp to datetime for dataframe with a \"datatime\" column with timestamp values. \"\"\"\n",
    "    ddf[\"datetime\"] = dd.to_datetime(ddf[\"datetime\"], unit='ms', errors='coerce')\n",
    "    ddf[\"datetime\"] = ddf[\"datetime\"].dt.tz_localize('UTC').dt.tz_convert('America/Bogota')\n",
    "    return ddf\n",
    "\n",
    "def preprocess_mobile(ddf: dd.DataFrame, final_cols: list, minlon , maxlon, minlat, maxlat): #needs work\n",
    "    \"\"\"Select only those points within an area of interest and process timestamp to datetime \n",
    "    for dataframe with a \"datatime\" column with timestamp values.\"\"\"\n",
    "    ddf = find_within_box(ddf, minlon, maxlon, minlat, maxlat)\n",
    "    ddf = convert_datetime(ddf)[final_cols]\n",
    "    df = ddf.compute()\n",
    "    return df\n",
    "\n",
    "def find_within_box(ddf, minlon, maxlon, minlat, maxlat):\n",
    "    \"\"\"Quick way to filter out points not in a particular rectangular region.\"\"\"\n",
    "    box=[minlon,minlat,maxlon,maxlat]\n",
    "    filtered_ddf = mobilkit.loader.crop_spatial(ddf, box).reset_index()\n",
    "    return filtered_ddf\n",
    "\n",
    "#### FUNCTIONS FOR PARQUET CONVERSION ####\n",
    "\n",
    "def write_to_pq(df, out_dir, filename): \n",
    "    table_name = f'{out_dir}{filename}.parquet'\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, table_name)\n",
    "\n",
    "def from_month_write_filter_days_to_pq(data_folder: str, month: str, year: str, out_dir:str):\n",
    "    day_dirs = glob.glob((data_folder + '*/'))\n",
    "    for i in tqdm(range(0,len(day_dirs)), desc=f'Files from {year} {month} processed'): \n",
    "        day_dir = day_dirs[i]\n",
    "        filepaths, day = get_files(data_folder, day_dir)\n",
    "        day_name = day.split('/')[0]\n",
    "        ddf = load_data(filepaths, initial_cols, sel_cols, final_cols)\n",
    "        df = preprocess_mobile(ddf, final_cols, minlon, maxlon, minlat, maxlat)\n",
    "        filename = f'bogota_area_{year}_{month}_{day_name}'\n",
    "        write_to_pq(df, out_dir, filename)\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Re-formatting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert tabular data to parquet\n",
    "For each day in each month, load the files for the Colombia mobile data, filter the pings that are roughly in the Bogota area, and process the datetime. Write each day as a parquet file with the year, month, and day information in the filename.\n",
    "\n",
    "More information [here](https://arrow.apache.org/docs/python/parquet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = f'{data_dir}data/'\n",
    "year = 'year=2019'\n",
    "data_year = f'{in_dir}{year}/'\n",
    "data_folders = glob.glob((data_year + '*/'))\n",
    "\n",
    "out_dir = f'{data_dir}data/parquet/'\n",
    "\n",
    "for i in range(0, len(data_folders)):\n",
    "    data_folder = data_folders[i]\n",
    "    month = data_folder.split(data_year)[1].split('/')[0]\n",
    "    #from_month_write_filter_days_to_pq(data_folder, month, year, out_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset from parquet\n",
    "Convert the parquet files into a dataset that is queryable\n",
    "\n",
    "More information [here](https://arrow.apache.org/docs/python/dataset.html#dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "pq_dir = f'{data_dir}data/parquet/'\n",
    "#pq_dir = f'{data_dir}data/test_parquet/'\n",
    "\n",
    "dataset = ds.dataset(pq_dir, format=\"parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the documentation \"Creating a Dataset object does not begin reading the data itself. If needed, it only crawls the directory to find all the file and infers the datasetâ€™s schema (by default from the first file).\" \n",
    "\n",
    "Lets look at some properties of the dataset, including what files it would be based on and some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.files)\n",
    "print(dataset.schema.to_string(show_field_metadata=False))\n",
    "fragments = list(dataset.get_fragments())\n",
    "print(fragments)\n",
    "#fragments.split_by_row_group()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Dataset.to_table() method we can read the dataset (or a portion of it) into a pyarrow Table. Depending on the dataset size this can require a lot of memory, so it's best to consider filtering the dataset first. For instance if we only want the user id column we can execute the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_user_stats(dataset):\n",
    "    table = dataset.to_table(columns=['uid', 'datetime']).to_pandas()\n",
    "    table_dd = dd.from_pandas(table, npartitions=10)\n",
    "    user_stats = mobilkit.stats.userStats(table_dd).compute()\n",
    "    return user_stats\n",
    "\n",
    "# note this takes quite some time (~20 minutes for two months of data)\n",
    "user_stats = compute_user_stats(dataset)\n",
    "user_stats.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out user stats for the whole dataset for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pings, min_days = 60, 10 \n",
    "output_filepath = f'{data_dir}/data/user_stats_2019_months1_2_60min_pings_10min_days.csv'\n",
    "output_folder = f'{data_dir}data/agg_data/'\n",
    "user_stats_filtered = user_stats[(user_stats['pings'] >= min_pings) & (user_stats['daysActive'] >= min_days)]\n",
    "print(f\"Based on {min_pings} ping and {min_days} day mininum cutoffs, kept {len(user_stats_filtered)} of a total of {len(user_stats)} users for this dataset.\")\n",
    "user_stats_filtered.to_csv(output_filepath, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter dataset by users that pass quality control \n",
    "Write out to parquet file for downstream analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids_pass_qc= list(user_stats_filtered['uid'])\n",
    "table = dataset.to_table(filter=ds.field('uid').isin(uids_pass_qc))\n",
    "data_for_qcd_users = f'{out_dir}bogota_area_year=2019_month=1and2_pass_qc.parquet'\n",
    "pq.write_table(table, data_for_qcd_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_qcd_users = f'{out_dir}bogota_area_year=2019_month=1and2_pass_qc.parquet'\n",
    "qc_user_data = dd.read_parquet(data_for_qcd_users)\n",
    "qc_user_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(qc_user_data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter by particular queries downstream, which is so cool! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('geo_mobile')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94266dc0016789288e396fdf5aae0a4b8003dfb3304fe52aa146b44e3e043f3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
